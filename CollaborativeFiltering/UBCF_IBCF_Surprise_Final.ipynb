{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cadbc0b6-1622-4c6b-8848-cbcea002e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "952e3c9e-0e61-4e2c-ba02-d5298a62e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/Users/kevinli/Downloads/filtered_small_df_v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfeccfe5-86a3-4701-8320-fcdb89eef81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336389, 22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fedeb4b-26ba-47a5-9c8a-0d344fd6962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ubcf = df[['user_id', 'business_id', 'stars_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3c4d2e7-7bc1-4f45-921c-3d1ba35c5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_counts = df_ubcf['user_id'].value_counts()\n",
    "users_with_enough_ratings = user_rating_counts[user_rating_counts >= 50].index.tolist()\n",
    "# Keep only the rows for users with at least 50 ratings\n",
    "df_ubcf = df_ubcf[df_ubcf['user_id'].isin(users_with_enough_ratings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de04ba-f4e9-45d4-8be6-be4da51646c5",
   "metadata": {},
   "source": [
    "## Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59cf850b-1e5a-49c7-a765-e35c119cb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming df_ubcf is your DataFrame with columns ['user_id', 'business_id', 'stars_x']\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ubcf[['user_id', 'business_id', 'stars_x']], reader)\n",
    "\n",
    "# Explicit train-test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state = 42)\n",
    "#trainset = trainset.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55621c8e-03a4-4364-aec9-e7bf2243970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Precision@5 (with ties): 0.8495265184362968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, trainset):\n",
    "        self.trainset = trainset\n",
    "        self.user_based_algo = KNNBasic(user_based=True)\n",
    "        self.item_based_algo = KNNBasic(user_based=False)\n",
    "    \n",
    "    def train(self):\n",
    "        self.user_based_algo.fit(self.trainset)\n",
    "        self.item_based_algo.fit(self.trainset)\n",
    "    \n",
    "    def get_top_n(self, predictions, n=10):\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, _, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est))\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "        return top_n\n",
    "    \n",
    "    def refine_with_item_based(self, top_n_recommendations):\n",
    "        final_recommendations = defaultdict(list)\n",
    "        for uid, items in top_n_recommendations.items():\n",
    "            try:\n",
    "                inner_uid = self.trainset.to_inner_uid(uid)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping refinement for {uid} as they are not in the training set.\")\n",
    "                continue\n",
    "            item_based_predictions = []\n",
    "            for iid, _ in items:\n",
    "                try:\n",
    "                    inner_iid = self.trainset.to_inner_iid(iid)\n",
    "                    est = self.item_based_algo.predict(uid, iid, verbose=False).est\n",
    "                    item_based_predictions.append((iid, est))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            item_based_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            final_recommendations[uid] = item_based_predictions[:5]\n",
    "        return final_recommendations\n",
    "\n",
    "class EvaluationSystem:\n",
    "    @staticmethod\n",
    "    def calculate_precision_at_k_with_ties(final_recommendations, testset, k=5):\n",
    "        test_ratings_by_user = defaultdict(list)\n",
    "        for uid, iid, rating in testset:\n",
    "            test_ratings_by_user[uid].append((iid, rating))\n",
    "        top_5_items_with_ties_by_user = defaultdict(set)\n",
    "        for uid, items_ratings in test_ratings_by_user.items():\n",
    "            sorted_items = sorted(items_ratings, key=lambda x: x[1], reverse=True)\n",
    "            top_ratings = sorted_items[:k] if len(sorted_items) > k else sorted_items\n",
    "            min_top_rating = min(top_ratings, key=lambda x: x[1])[1]\n",
    "            top_5_items_with_ties_by_user[uid] = {item for item, rating in sorted_items if rating >= min_top_rating}\n",
    "        hits = 0\n",
    "        total_predictions = 0\n",
    "        for uid, recommendations in final_recommendations.items():\n",
    "            recommended_item_ids = {iid for iid, _ in recommendations[:5]}\n",
    "            actual_top_items = top_5_items_with_ties_by_user.get(uid, set())\n",
    "            hits += len(recommended_item_ids.intersection(actual_top_items))\n",
    "            total_predictions += len(recommended_item_ids)\n",
    "        precision_at_k_with_ties = hits / total_predictions if total_predictions > 0 else 0\n",
    "        return precision_at_k_with_ties\n",
    "\n",
    "# Example usage:\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "#train_data = Dataset.load_from_df(train_df[['user_id', 'business_id', 'stars_y']], reader)\n",
    "#trainset = train_data.build_full_trainset()\n",
    "\n",
    "rec_sys = RecommendationSystem(trainset)\n",
    "rec_sys.train()\n",
    "\n",
    "# Save the trained models\n",
    "with open('recommendation_system.pkl', 'wb') as file:\n",
    "    pickle.dump(rec_sys, file)\n",
    "\n",
    "user_based_predictions = rec_sys.user_based_algo.test(testset)\n",
    "top_n_user_based = rec_sys.get_top_n(user_based_predictions, n=10)\n",
    "final_recommendations = rec_sys.refine_with_item_based(top_n_user_based)\n",
    "\n",
    "eval_sys = EvaluationSystem()\n",
    "precision = eval_sys.calculate_precision_at_k_with_ties(final_recommendations, testset, k=5)\n",
    "print(f\"Precision@5 (with ties): {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa606b-2449-4b39-8871-f10b2df34a1b",
   "metadata": {},
   "source": [
    "## Cascade Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21ffc7-e643-41f7-8608-206bb364f5f1",
   "metadata": {},
   "source": [
    "#### Train User-Based and Item-Based Models\n",
    "Functions: KNNBasic(user_based=True) and KNNBasic(user_based=False)\n",
    "\n",
    "Purpose: Initializes the collaborative filtering algorithms for user-based and item-based recommendations, respectively.\n",
    "Use: These functions are key to creating models that can predict item ratings based on similar users (user-based) or similar items (item-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e6307-fa9f-4924-8e94-d2454cd1b474",
   "metadata": {},
   "source": [
    "#### Generate Limited Anti-Testset\n",
    "Function: Custom function using trainset.all_users(), trainset.all_items(), and sampling logic.\n",
    "\n",
    "Purpose: Reduces the computational complexity by limiting the number of items to predict per user, focusing on a manageable subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035939b-2436-4bc5-8dc6-80109fcebbde",
   "metadata": {},
   "source": [
    "#### Predict Ratings with User-Based Model and Select Top N\n",
    "Function: user_based_algo.test(limited_testset) and top_n\n",
    "\n",
    "Purpose: Generates predictions for the limited set of user-item pairs using the user-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c5b53-98f5-471d-a0b3-25bdcff5b1dd",
   "metadata": {},
   "source": [
    "#### Item Based Refinement\n",
    "\n",
    "For each recommended item in the top 10 user list: \n",
    "Use the item-based model to predict the user's rating for the item. This step leverages the item-based model's understanding of item similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91e018-5d47-419e-8f82-7963cae58d6b",
   "metadata": {},
   "source": [
    "### Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cff39e1b-6853-49a2-9bdb-0e1cce659fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0729\n",
      "RMSE: 1.0729\n",
      "User-Based CF RMSE: 1.0729180967373162\n",
      "Item-Based CF RMSE: 1.0729180967373162\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming df_ubcf is your DataFrame with columns ['user_id', 'business_id', 'stars_x']\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ubcf[['user_id', 'business_id', 'stars_x']], reader)\n",
    "\n",
    "# Explicit train-test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Define algorithms for user-based and item-based collaborative filtering\n",
    "user_based_algo = KNNBasic(user_based=True)\n",
    "item_based_algo = KNNBasic(user_based=False)\n",
    "\n",
    "# Train both models on the training set\n",
    "user_based_algo.fit(trainset)\n",
    "item_based_algo.fit(trainset)\n",
    "\n",
    "# Test both models on the testing set\n",
    "user_based_predictions = user_based_algo.test(testset)\n",
    "item_based_predictions = item_based_algo.test(testset)\n",
    "\n",
    "# Calculate and print the RMSE for both models\n",
    "user_based_rmse = rmse(user_based_predictions)\n",
    "item_based_rmse = rmse(item_based_predictions)\n",
    "\n",
    "print(f\"User-Based CF RMSE: {user_based_rmse}\")\n",
    "print(f\"Item-Based CF RMSE: {item_based_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c3d73ab-192e-475c-a717-9917ac14ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store user_id, business_id, and stars_x\n",
    "ratings_list = []\n",
    "# Iterate through each rating in the trainset\n",
    "for uid, iid, rating in trainset.all_ratings():\n",
    "    # Convert internal IDs to the raw IDs\n",
    "    raw_uid = trainset.to_raw_uid(uid)\n",
    "    raw_iid = trainset.to_raw_iid(iid)\n",
    "    # Append the raw IDs and rating to the list\n",
    "    ratings_list.append((raw_uid, raw_iid, rating))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_ratings = pd.DataFrame(ratings_list, columns=['user_id', 'business_id', 'stars_x'])\n",
    "df_ratings.to_csv('trainset_rec_ind.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1822f509-127f-4e86-acc2-0de91fdc4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited_testset = pd.DataFrame(limited_testset, columns=['user_id', 'business_id', 'predicted_rating'])\n",
    "df_limited_testset.to_csv('testset_ind.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719f5c1-87c0-40fd-9c6c-76d7ba72ecae",
   "metadata": {},
   "source": [
    "## Final Cascade Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78d12793-cbd3-457e-bab5-2f79fe787acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_ubcf is your DataFrame with columns ['user_id', 'business_id', 'stars_x']\n",
    "\n",
    "# Step 1: Filter users with at least 20 ratings\n",
    "user_rating_counts = df_ubcf['user_id'].value_counts()\n",
    "users_with_enough_ratings = user_rating_counts[user_rating_counts >= 20].index.tolist()\n",
    "\n",
    "# Keep only the rows for users with at least 20 ratings\n",
    "filtered_df = df_ubcf[df_ubcf['user_id'].isin(users_with_enough_ratings)]\n",
    "\n",
    "# Step 2: Sample 20 ratings for each of these users to create a test set\n",
    "def sample_test_set(df, n=20):\n",
    "    # Randomly sample n ratings for each user to form the test set\n",
    "    test_df = df.groupby('user_id').sample(n=n, random_state=42)\n",
    "    return test_df\n",
    "\n",
    "test_df = sample_test_set(filtered_df)\n",
    "# Exclude these test set ratings from the original filtered DataFrame to form the training set\n",
    "train_df = filtered_df.drop(test_df.index)\n",
    "\n",
    "# Step 3: Load the train and test DataFrames into Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load the training set\n",
    "train_data = Dataset.load_from_df(train_df[['user_id', 'business_id', 'stars_y']], reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "\n",
    "# Since Surprise doesn't directly support loading a test set from a DataFrame,\n",
    "# convert the test DataFrame into the list of tuples expected by the test method in Surprise\n",
    "testset = [tuple(x) for x in test_df[['user_id', 'business_id', 'stars_y']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01f4de01-99f6-4b79-ad25-33ee8f78f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited_testset = pd.DataFrame(testset, columns=['user_id', 'business_id', 'predicted_rating'])\n",
    "df_limited_testset.to_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47149a64-53ea-4c23-9a55-dfe84d7eebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating explicit df dataframe \n",
    "# Initialize an empty list to store user_id, business_id, and stars_x\n",
    "ratings_list = []\n",
    "# Iterate through each rating in the trainset\n",
    "for uid, iid, rating in trainset.all_ratings():\n",
    "    # Convert internal IDs to the raw IDs\n",
    "    raw_uid = trainset.to_raw_uid(uid)\n",
    "    raw_iid = trainset.to_raw_iid(iid)\n",
    "    # Append the raw IDs and rating to the list\n",
    "    ratings_list.append((raw_uid, raw_iid, rating))\n",
    "# Convert the list to a DataFrame\n",
    "df_ratings = pd.DataFrame(ratings_list, columns=['user_id', 'business_id', 'stars_x'])\n",
    "df_ratings.head()  # Display the first few rows of the dataframe\n",
    "df_ratings.to_csv('trainset_rec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3adf4fc-4955-459a-b909-93f7d0f262ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Define algorithms for user-based and item-based collaborative filtering\n",
    "user_based_algo = KNNBasic(user_based=True)\n",
    "item_based_algo = KNNBasic(user_based=False)\n",
    "\n",
    "# Train both models on the training set\n",
    "user_based_algo.fit(trainset)\n",
    "item_based_algo.fit(trainset)\n",
    "\n",
    "# Function to get top N recommendations from predictions\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, _, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "# Generate predictions for the testset using the user-based model\n",
    "user_based_predictions = user_based_algo.test(testset)\n",
    "top_n_user_based = get_top_n(user_based_predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c70d2053-683a-485a-8ddc-0cef68c5def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping refinement for -1MF2tosrw2WcCxeVNk81Q as they are not in the training set.\n",
      "Skipping refinement for -A8NWVsLSAQX_XTqt4WPmg as they are not in the training set.\n",
      "Skipping refinement for -Od0vvWj3RISQ0pNBGqXnQ as they are not in the training set.\n",
      "Skipping refinement for -Rocdfu1eqSbyqCEBvOzDw as they are not in the training set.\n",
      "Skipping refinement for -T-i6BSAeSuqLGqIQ-u3wQ as they are not in the training set.\n",
      "Skipping refinement for -VOCFO1QSCjAxl_0LIrZ9Q as they are not in the training set.\n",
      "Skipping refinement for -dcWUGQY6uDGQ_FHRySDeQ as they are not in the training set.\n",
      "Skipping refinement for -mLeT8Ya1D2USbysd6yjrQ as they are not in the training set.\n",
      "Skipping refinement for -wL-2J0enMz1DDPgPlLHwA as they are not in the training set.\n",
      "Skipping refinement for 01EE-OfsMFnZJhfNQsp2vg as they are not in the training set.\n",
      "Skipping refinement for 0DZQA74K8IodQfBEMxJbQw as they are not in the training set.\n",
      "Skipping refinement for 0Ersa47HgrkiHD8GhqIpgw as they are not in the training set.\n",
      "Skipping refinement for 0eOq2tJa2tmPxWcqrsg0Qw as they are not in the training set.\n",
      "Skipping refinement for 0zDHiDbj79uBOy3OAcE25A as they are not in the training set.\n",
      "Skipping refinement for 1-eelqq4Qr90xxWcLLbrqA as they are not in the training set.\n",
      "Skipping refinement for 117a3aPbQMFcwWvFLPQiyQ as they are not in the training set.\n",
      "Skipping refinement for 16t0PNNu5N-fHe7TNB02FA as they are not in the training set.\n",
      "Skipping refinement for 1M78_w4J9f5S8xmUVYyxdQ as they are not in the training set.\n",
      "Skipping refinement for 1SWleF9HBQ3zcgHcTKbDLQ as they are not in the training set.\n",
      "Skipping refinement for 1U-U6abdncybg6c2TRl0FQ as they are not in the training set.\n",
      "Skipping refinement for 1_2DkgyvN4Vy7gkb-XRA-w as they are not in the training set.\n",
      "Skipping refinement for 1i1pFuuBzOcOwiM3kkYwLQ as they are not in the training set.\n",
      "Skipping refinement for 1mwlAJsbpb8lRRTCtBOKjw as they are not in the training set.\n",
      "Skipping refinement for 1rCtEc1hzFF4m5zJ4wngHg as they are not in the training set.\n",
      "Skipping refinement for 2BNYBKaBMOB28_jK-Bn4fQ as they are not in the training set.\n",
      "Skipping refinement for 2Gg5MdVTcNmU4A6KvNOKCg as they are not in the training set.\n",
      "Skipping refinement for 2HGxY9bhMmXFVYtk-rqSbw as they are not in the training set.\n",
      "Skipping refinement for 2Po0iOtcLe2KwcqfduMv7w as they are not in the training set.\n",
      "Skipping refinement for 2aQPINTV7nXBYkQO0NoNbQ as they are not in the training set.\n",
      "Skipping refinement for 37bjmw5F6FwMnaTRsrEhYg as they are not in the training set.\n",
      "Skipping refinement for 3HKvvtmq4DUyuyXklaKiyA as they are not in the training set.\n",
      "Skipping refinement for 3K-pYkV5b2CexdCw3qvrWw as they are not in the training set.\n",
      "Skipping refinement for 3OqTsRD7b7gkhhGks31XrA as they are not in the training set.\n",
      "Skipping refinement for 3P4i3KsrexJvj6p7xpU1_Q as they are not in the training set.\n",
      "Skipping refinement for 3abnbrawtIo8sutS-kb0oA as they are not in the training set.\n",
      "Skipping refinement for 4-cD3XkmNIIiGiUFtfLN7A as they are not in the training set.\n",
      "Skipping refinement for 40HOBub-FknKR20RDe8UzQ as they are not in the training set.\n",
      "Skipping refinement for 4I0CNcNYC64DzlP97I6stA as they are not in the training set.\n",
      "Skipping refinement for 4Qtv00D5xteyBTpbF0qtkA as they are not in the training set.\n",
      "Skipping refinement for 4fTm8qhzNnSXYHugs-Jcrw as they are not in the training set.\n",
      "Skipping refinement for 4nMwZ_FOINd-qwM2PRJWeQ as they are not in the training set.\n",
      "Skipping refinement for 5CpIu_UkdVfrri5t2nKBnQ as they are not in the training set.\n",
      "Skipping refinement for 5Fk3pRHy5iqu3Bvo_zj5DA as they are not in the training set.\n",
      "Skipping refinement for 5HkEIyJoOmC2Zioh74fIFQ as they are not in the training set.\n",
      "Skipping refinement for 5NsB7O10FRrrHVAP1Z9RnQ as they are not in the training set.\n",
      "Skipping refinement for 5kBCV_Fu4PEInxK_MRmstw as they are not in the training set.\n",
      "Skipping refinement for 6UunqF-6SDRhN9Avx_Kh9w as they are not in the training set.\n",
      "Skipping refinement for 6aIApznRIGiWiJXoR8gdog as they are not in the training set.\n",
      "Skipping refinement for 6cPC5XzBoS1IAM-bS2RAJQ as they are not in the training set.\n",
      "Skipping refinement for 6e1IgA1NrrNno9bzxq-Gpg as they are not in the training set.\n",
      "Skipping refinement for 7-mA5iLgHJt6LRbi8ex09Q as they are not in the training set.\n",
      "Skipping refinement for 71p7Qmz3PNjfrMqpAzgK3Q as they are not in the training set.\n",
      "Skipping refinement for 7EEVarOuN0AqdwdkN7io-w as they are not in the training set.\n",
      "Skipping refinement for 7Eq71a1cwHZ-T-k9pusdlw as they are not in the training set.\n",
      "Skipping refinement for 7NEmC-Q2Ikx8qG48WnIr2g as they are not in the training set.\n",
      "Skipping refinement for 7UVH0pYhL1Mpvdt4rGFBMA as they are not in the training set.\n",
      "Skipping refinement for 7fK3qPtwzimmy7p9tJdkiQ as they are not in the training set.\n",
      "Skipping refinement for 8-dG2HZtFc0dCoD6dCqWdg as they are not in the training set.\n",
      "Skipping refinement for 8335cvG3_f_fxDwKl5MdjQ as they are not in the training set.\n",
      "Skipping refinement for 89rD-wRrTSMDSlewnEuGww as they are not in the training set.\n",
      "Skipping refinement for 8Ktnp0D9x0VvW1hztdIAyg as they are not in the training set.\n",
      "Skipping refinement for 8Oqwep1RrjcpP2OyAMiNRg as they are not in the training set.\n",
      "Skipping refinement for 8gg0mL6yj4vOL3f6mAOEgA as they are not in the training set.\n",
      "Skipping refinement for 99vwQLmpqU5O1Tq-vsQPPw as they are not in the training set.\n",
      "Skipping refinement for 9FkWHjASv64oj2MiezEBlw as they are not in the training set.\n",
      "Skipping refinement for 9LQ9JgyAhl_6VRHJTsNL7w as they are not in the training set.\n",
      "Skipping refinement for 9RZ06DUYQuLQanpGSqGddw as they are not in the training set.\n",
      "Skipping refinement for 9eNliXKTZTI3yFR4OO12cQ as they are not in the training set.\n",
      "Skipping refinement for 9g2yOSgIpHq3gcpX756HeQ as they are not in the training set.\n",
      "Skipping refinement for A-PODWXgcu7Ln2XjT0DLUQ as they are not in the training set.\n",
      "Skipping refinement for A2xip8ebxVcxpmRIOY3G4g as they are not in the training set.\n",
      "Skipping refinement for A3TLdhgsYkYbx3SsDdffzA as they are not in the training set.\n",
      "Skipping refinement for AP7o6yRkjkqOveZxHg9seA as they are not in the training set.\n",
      "Skipping refinement for AjA0sLYdZVj-NRMNYeNOnQ as they are not in the training set.\n",
      "Skipping refinement for AwUdeKvMFumMhgaOxrvPnA as they are not in the training set.\n",
      "Skipping refinement for BE40dCrMXjhj8JU8jgQlDg as they are not in the training set.\n",
      "Skipping refinement for BcUnmh4KaHfu7d_pzT-dkw as they are not in the training set.\n",
      "Skipping refinement for BehuW6rGBjox4uLoKa0JGQ as they are not in the training set.\n",
      "Skipping refinement for BnJRHXyU5rsAdLrtcg-GMw as they are not in the training set.\n",
      "Skipping refinement for Bzrsyao3yYX8dnLV0GADWg as they are not in the training set.\n",
      "Skipping refinement for CIPjhmRzmFMXcBNAkNauwg as they are not in the training set.\n",
      "Skipping refinement for CgtJDze587N3th0TnkJTNg as they are not in the training set.\n",
      "Skipping refinement for CkrwgoY_Cdbj60IQXH-AQw as they are not in the training set.\n",
      "Skipping refinement for D4e3KTJ-XjdfDurc2mBR0Q as they are not in the training set.\n",
      "Skipping refinement for DHJJPj7ZlhJ_AmpXPJ7g0g as they are not in the training set.\n",
      "Skipping refinement for DJNhl3mlzTJkmsl61ICA-Q as they are not in the training set.\n",
      "Skipping refinement for DNzgJRdL6kXyQcXABaPEvA as they are not in the training set.\n",
      "Skipping refinement for D_pSCCzes0LwGuYEwyk5fA as they are not in the training set.\n",
      "Skipping refinement for DvFDfCst0pWyvjG-xkWFTQ as they are not in the training set.\n",
      "Skipping refinement for DvQJNI0NwgM9fjeqlug2kQ as they are not in the training set.\n",
      "Skipping refinement for E2POu4WF99VvKdzYPL0Akg as they are not in the training set.\n",
      "Skipping refinement for E9ie-fl14jUToKby8eVd4A as they are not in the training set.\n",
      "Skipping refinement for ELM_r65cq1hK1AtTwbGwgA as they are not in the training set.\n",
      "Skipping refinement for EQAghmtWV39rcoOu4Z6Zqw as they are not in the training set.\n",
      "Skipping refinement for ERCnx8jFmt_R56Z0gIee0w as they are not in the training set.\n",
      "Skipping refinement for EZgoswReHYNzjeIGFI0Dzw as they are not in the training set.\n",
      "Skipping refinement for Et3Mf9CRUKHkG0OnWmHoxA as they are not in the training set.\n",
      "Skipping refinement for EvVbekEpWXAeMNt3mQlAjg as they are not in the training set.\n",
      "Skipping refinement for F26BuGco0MEqi08QQrzMkA as they are not in the training set.\n",
      "Skipping refinement for F39p180B9y63sFKHMTfBxg as they are not in the training set.\n",
      "Skipping refinement for FdPrn04vvhtJPum9nPCUeQ as they are not in the training set.\n",
      "Skipping refinement for Fivx-gzIXwR-uEYF9FPr_g as they are not in the training set.\n",
      "Skipping refinement for G1VmrB6jB6_HE8dlQANfVg as they are not in the training set.\n",
      "Skipping refinement for G8z-JeJ_booU3CRHSOHkxA as they are not in the training set.\n",
      "Skipping refinement for GQJn9zM03Yt3ynn2VvKltw as they are not in the training set.\n",
      "Skipping refinement for GYPUqYM9i4YgELbQRSiO2Q as they are not in the training set.\n",
      "Skipping refinement for GgGen6L5WZowpOEKuu1ZfQ as they are not in the training set.\n",
      "Skipping refinement for GjPgYTpipSw0a1zd0Ypwcg as they are not in the training set.\n",
      "Skipping refinement for Gu_i42c5nD8uSYxIJeFNOA as they are not in the training set.\n",
      "Skipping refinement for Gu_zJwYF3dGvOgUp7E6GAA as they are not in the training set.\n",
      "Skipping refinement for HXsWRKMklalnFTy8zC_OPw as they are not in the training set.\n",
      "Skipping refinement for Hixk-gWc13XwvExs5lFmIA as they are not in the training set.\n",
      "Skipping refinement for HkS7H9H8C2fqZkzvrdoUBw as they are not in the training set.\n",
      "Skipping refinement for Hx8xv2o-yhNDZ4kC8U9gXA as they are not in the training set.\n",
      "Skipping refinement for I28lom0iOPibv5iN7BM0BQ as they are not in the training set.\n",
      "Skipping refinement for IA2y_ztIiRKvd-AQFs8hRg as they are not in the training set.\n",
      "Skipping refinement for IoK3XlDg5n8KzsrvXLT79w as they are not in the training set.\n",
      "Skipping refinement for Iw6uScVAE_MA7cHZey67yg as they are not in the training set.\n",
      "Skipping refinement for Iz3UXfJbwXOIx66-vdEUmQ as they are not in the training set.\n",
      "Skipping refinement for JMuncPRtxhMAX8XweA8gDA as they are not in the training set.\n",
      "Skipping refinement for JafROPjlVHrw5yndL93l5w as they are not in the training set.\n",
      "Skipping refinement for JdwEau_cSAZjI2kKslEQXA as they are not in the training set.\n",
      "Skipping refinement for JmqXHl4hsfzRZGDg58hxaA as they are not in the training set.\n",
      "Skipping refinement for K5eF2a5mOERWRGfgLH9P1w as they are not in the training set.\n",
      "Skipping refinement for K8VSrOOCdXS9VX5S66W4GQ as they are not in the training set.\n",
      "Skipping refinement for KJF_5vG6YLaVeU7N8aDALQ as they are not in the training set.\n",
      "Skipping refinement for KKHwNzxmz3OXkZRyV75r6A as they are not in the training set.\n",
      "Skipping refinement for KTY8FVt4FYj-hvVRW_Qllw as they are not in the training set.\n",
      "Skipping refinement for Kig7LAiwLafFKFcZW8BATA as they are not in the training set.\n",
      "Skipping refinement for KjJaXF23qJxyy-ckZ0Svew as they are not in the training set.\n",
      "Skipping refinement for Knm4qxKP9y3qNFqkHvnN5w as they are not in the training set.\n",
      "Skipping refinement for LgxSFmd8uv4OTIzkiQ20qg as they are not in the training set.\n",
      "Skipping refinement for LvXiX9mEI3ttxosoK0tVPQ as they are not in the training set.\n",
      "Skipping refinement for M9BgUhUEkAQIXhOa4ikEKQ as they are not in the training set.\n",
      "Skipping refinement for M9q-32U9Tgi2UqMoZYJrBw as they are not in the training set.\n",
      "Skipping refinement for MwVqMIGChaj9ktX55UBONQ as they are not in the training set.\n",
      "Skipping refinement for N67MPxXk-RwDCaJVmsLWLw as they are not in the training set.\n",
      "Skipping refinement for NF2plo71mxnUOEsV7L17dw as they are not in the training set.\n",
      "Skipping refinement for NS8h971tPbGXDLqMky_hJg as they are not in the training set.\n",
      "Skipping refinement for NejwGXWgAt2mA44mRT0obg as they are not in the training set.\n",
      "Skipping refinement for Nro6ABevZFu-8TFDKS-5bw as they are not in the training set.\n",
      "Skipping refinement for Nzo06_sG-DEhYakUKWdSZQ as they are not in the training set.\n",
      "Skipping refinement for O5J4Cc1U7tXT-6MoogLWrw as they are not in the training set.\n",
      "Skipping refinement for O6e_dHvcb_UnsjmCXcQu_w as they are not in the training set.\n",
      "Skipping refinement for OXwlmFPcSe9TVAdDORJnAw as they are not in the training set.\n",
      "Skipping refinement for OZR8LkzaVYU4L7fT4BKHOw as they are not in the training set.\n",
      "Skipping refinement for ObpMR3OjoJDmiTcObXPZPQ as they are not in the training set.\n",
      "Skipping refinement for P3oTNMEOVLCrbtDTgxH8Tw as they are not in the training set.\n",
      "Skipping refinement for PEhyruu0fJtayrVJ7qYzaQ as they are not in the training set.\n",
      "Skipping refinement for PIGRRUZlIsssfuLWkZ4vyw as they are not in the training set.\n",
      "Skipping refinement for PIWe5VYOcgzum6ush3nycw as they are not in the training set.\n",
      "Skipping refinement for PV7wRAulus-RzEBoJmLyVQ as they are not in the training set.\n",
      "Skipping refinement for PfMObQ3LxKtfLwKorzgRXg as they are not in the training set.\n",
      "Skipping refinement for QCrW0zYvHcWjtN0WqdR9yg as they are not in the training set.\n",
      "Skipping refinement for QEC6cQAf7fdQopqt1atLkQ as they are not in the training set.\n",
      "Skipping refinement for QEHGOdv-Tr96DsZ12U2csQ as they are not in the training set.\n",
      "Skipping refinement for QJpymN4GQPaOz5s8knpPDg as they are not in the training set.\n",
      "Skipping refinement for QigcKGefGkJMhVTVXpAvCg as they are not in the training set.\n",
      "Skipping refinement for Qlvnhz7kIQ69jzgE4dEFlQ as they are not in the training set.\n",
      "Skipping refinement for RHBEH_PAHLXaX8qeAe8_gQ as they are not in the training set.\n",
      "Skipping refinement for ROBfuofw3iVYLJ7R9mqcbQ as they are not in the training set.\n",
      "Skipping refinement for ROdeRSIcXM2JyM5LuRCS_A as they are not in the training set.\n",
      "Skipping refinement for RRtMl0kS2ntfoxcQPJcmZA as they are not in the training set.\n",
      "Skipping refinement for Rcrg_WMhz6KUu-uDhEt38g as they are not in the training set.\n",
      "Skipping refinement for Rf9lBBipUCYeUjPdPx9O8w as they are not in the training set.\n",
      "Skipping refinement for Rgf3LmaAm59puuS2q6rUQw as they are not in the training set.\n",
      "Skipping refinement for Rl3sJHcN8YGL1JvmGlHomg as they are not in the training set.\n",
      "Skipping refinement for SC28VXF_U9i06HV1ChyXag as they are not in the training set.\n",
      "Skipping refinement for SIjFTIdPhl5QMJEayAZN1A as they are not in the training set.\n",
      "Skipping refinement for SkrqglaWKXpeuFMniJO87Q as they are not in the training set.\n",
      "Skipping refinement for SruccJ9YE2lzkaujYPHwzQ as they are not in the training set.\n",
      "Skipping refinement for SsjCEM94VqJArdGq60DDaw as they are not in the training set.\n",
      "Skipping refinement for SyB7SoTx7U6sJb-Z2QF7wQ as they are not in the training set.\n",
      "Skipping refinement for T7BPbDZr7YDDAl1S9Xw_og as they are not in the training set.\n",
      "Skipping refinement for TG3KMNNJVDGHB_nA3MpkMw as they are not in the training set.\n",
      "Skipping refinement for TGJuVENc334nOml3XvGn2g as they are not in the training set.\n",
      "Skipping refinement for TGnn73j8T9yiJyeeh47fpg as they are not in the training set.\n",
      "Skipping refinement for TSlfAcIl1OLFGhD1jCB3gw as they are not in the training set.\n",
      "Skipping refinement for Tok5zBSJdWUhew9rqVHIeA as they are not in the training set.\n",
      "Skipping refinement for U-qdaQjZhBJdjndXbN7bnA as they are not in the training set.\n",
      "Skipping refinement for U9Rf9MTQJz7LqNl6A5kZDQ as they are not in the training set.\n",
      "Skipping refinement for UD-8cTOi8Bac3SZ-TBk3xw as they are not in the training set.\n",
      "Skipping refinement for UKjsdQvypama0X_henp77A as they are not in the training set.\n",
      "Skipping refinement for UMleWTnd-k0ZsCaXqIU9oA as they are not in the training set.\n",
      "Skipping refinement for URXhuvYnM0ZhS0T9oN6JvA as they are not in the training set.\n",
      "Skipping refinement for UdyrulYllDxVNmX-32FLFA as they are not in the training set.\n",
      "Skipping refinement for Ueq3WKgTuj0EKA-RVqHwYw as they are not in the training set.\n",
      "Skipping refinement for UimkcnVLBn8XUmU8pKJk2A as they are not in the training set.\n",
      "Skipping refinement for UkBI4VW3CwLvIgXaiiLdig as they are not in the training set.\n",
      "Skipping refinement for VJ_ndFgd1uR_0NhPOL2NnQ as they are not in the training set.\n",
      "Skipping refinement for VQ3J0tYADb5SjCqUiU0StQ as they are not in the training set.\n",
      "Skipping refinement for VXsXnyFqUiDUSxHMnj2L_A as they are not in the training set.\n",
      "Skipping refinement for Vgx_0jPlJ-ji1VvMmWL56A as they are not in the training set.\n",
      "Skipping refinement for W1eHcjd6kwKAzeFwDoOuJg as they are not in the training set.\n",
      "Skipping refinement for WeP4aqaeGvIQAmaqC7o1Vw as they are not in the training set.\n",
      "Skipping refinement for WemlxLqdv96cWWPJgcvNng as they are not in the training set.\n",
      "Skipping refinement for WfYGT0-TvctR6t5NGVOJsQ as they are not in the training set.\n",
      "Skipping refinement for WkaS0_JH8nn30zu8ew2Scw as they are not in the training set.\n",
      "Skipping refinement for Wmaml1zhE9JhCGKx7Z43fw as they are not in the training set.\n",
      "Skipping refinement for WvoF-O530mUOArrB5mA6AA as they are not in the training set.\n",
      "Skipping refinement for XMHRH9_8T8HhjQMS81cdCw as they are not in the training set.\n",
      "Skipping refinement for XNybG_RQzI7BECAep8LZaQ as they are not in the training set.\n",
      "Skipping refinement for Xi1fNcAQ0aORKCV-eTfNXQ as they are not in the training set.\n",
      "Skipping refinement for XkLr9hQUN6OSeILrOEOIOg as they are not in the training set.\n",
      "Skipping refinement for XqKrvSHKHRwrUxDVxMihGg as they are not in the training set.\n",
      "Skipping refinement for Y65r4NYMdzYvhMoBIfqNTg as they are not in the training set.\n",
      "Skipping refinement for Y9CCrqhYue2Eh2eBiH7KiQ as they are not in the training set.\n",
      "Skipping refinement for YNgQE1Ky332Gj8KVJ62eZA as they are not in the training set.\n",
      "Skipping refinement for YilexQQLMeaG9MboB8-9Iw as they are not in the training set.\n",
      "Skipping refinement for YvsClQ9-iK8XYuQj8AG8fQ as they are not in the training set.\n",
      "Skipping refinement for Z19QkDdtqhl7w88AR_ENXA as they are not in the training set.\n",
      "Skipping refinement for Z3YttNwgv1jAgDY9o5TDYw as they are not in the training set.\n",
      "Skipping refinement for Z7rVo0GOLa8cPAe3PciZmg as they are not in the training set.\n",
      "Skipping refinement for Z9pFyjHyEyLPqmbaC8vDBA as they are not in the training set.\n",
      "Skipping refinement for ZBUSFEyaulM6OXB4ra2E6A as they are not in the training set.\n",
      "Skipping refinement for ZHiVNK_5ytWSSR9HGr0iIQ as they are not in the training set.\n",
      "Skipping refinement for ZJ7evspASw1uOhgxq-gojQ as they are not in the training set.\n",
      "Skipping refinement for ZVu07cZy8ABS8V4QFMLqLg as they are not in the training set.\n",
      "Skipping refinement for Zl1fJNTzNULZiqBRShxUYA as they are not in the training set.\n",
      "Skipping refinement for ZptGk-V-K-jWNm0O18PuQw as they are not in the training set.\n",
      "Skipping refinement for Zqu-tMR9CHjA44sU7I5bXA as they are not in the training set.\n",
      "Skipping refinement for ZwGCiUXTqFEdbmzQWHmCVw as they are not in the training set.\n",
      "Skipping refinement for _3NuVuyphd-oBoupXAK4Dg as they are not in the training set.\n",
      "Skipping refinement for _4Xyu7-PLiZt2n-vljk9Ag as they are not in the training set.\n",
      "Skipping refinement for _BSD3T4KxmxIUnMTHEVj-g as they are not in the training set.\n",
      "Skipping refinement for _JNFhqt0aE70sONOOiCZ1w as they are not in the training set.\n",
      "Skipping refinement for _YURp-Tv9UoI3sePHpceoA as they are not in the training set.\n",
      "Skipping refinement for _dmNxWOwzQiqsB47tT5l5Q as they are not in the training set.\n",
      "Skipping refinement for _zKU-UD-R9SapkGroqLI6A as they are not in the training set.\n",
      "Skipping refinement for a1pu9OpkUiqTLkvl-P39vw as they are not in the training set.\n",
      "Skipping refinement for adMjyYAea_A0a36MrR5zBg as they are not in the training set.\n",
      "Skipping refinement for akPq-q-N8h-O64p4TPuqSQ as they are not in the training set.\n",
      "Skipping refinement for ap64wrW1-fSox3G8oM5Umg as they are not in the training set.\n",
      "Skipping refinement for b80BrMm6R5VYjj21gOQGPA as they are not in the training set.\n",
      "Skipping refinement for bQJJaQ_urivHUD0uUpj20A as they are not in the training set.\n",
      "Skipping refinement for bbtc1OcdwTOsz3i7WwpHGA as they are not in the training set.\n",
      "Skipping refinement for bi76Ih6tRv-ghYWFCJ0dag as they are not in the training set.\n",
      "Skipping refinement for bqmenudQMdOIbVCNese9Mg as they are not in the training set.\n",
      "Skipping refinement for brBOuZGPtkwkVQor-pvl1A as they are not in the training set.\n",
      "Skipping refinement for c-FDQvav6aIQrdb9589-CQ as they are not in the training set.\n",
      "Skipping refinement for c7nC1NiMplU9RXmmLvBcUg as they are not in the training set.\n",
      "Skipping refinement for cMoQqT_j_FD1gO6EEZzOIg as they are not in the training set.\n",
      "Skipping refinement for c_8BfmEVOUyPtHKK62H8dA as they are not in the training set.\n",
      "Skipping refinement for cixVJOekSGAPscF0F5Blqw as they are not in the training set.\n",
      "Skipping refinement for cpFDkUwNInBzgJ4a1pbyvA as they are not in the training set.\n",
      "Skipping refinement for d4SQBpFewh8dKkXD69WSrQ as they are not in the training set.\n",
      "Skipping refinement for dBeJP7N30_HqHyRDaYA3zg as they are not in the training set.\n",
      "Skipping refinement for dQZOOYT8fi7KRlj5drsySQ as they are not in the training set.\n",
      "Skipping refinement for d_Blgv9PgXX5CVnFwpURjw as they are not in the training set.\n",
      "Skipping refinement for ddc5DGUyRy3DA9KRlOyQSA as they are not in the training set.\n",
      "Skipping refinement for dkQh1i0EIAAv-SIM4zWACw as they are not in the training set.\n",
      "Skipping refinement for dr_mce0J8Q-BLLOA23KuAQ as they are not in the training set.\n",
      "Skipping refinement for dyzaphUiajbO48x-OpZnTQ as they are not in the training set.\n",
      "Skipping refinement for ePmanjMTkYwpO65_9_fwQA as they are not in the training set.\n",
      "Skipping refinement for eR6g04ytliXmeQyEI2rUow as they are not in the training set.\n",
      "Skipping refinement for eWFkABNBwFpwzNEUo_sb2g as they are not in the training set.\n",
      "Skipping refinement for edlf0nEAsld7DX4yCKyXHw as they are not in the training set.\n",
      "Skipping refinement for emvvPgIEOLruyz0sQZ8OYg as they are not in the training set.\n",
      "Skipping refinement for eod1mk6q1p155DTZqLoRqg as they are not in the training set.\n",
      "Skipping refinement for eskGH1Z0IBHxrz4s4An_Cw as they are not in the training set.\n",
      "Skipping refinement for evSTPfN2RjZwKhLIWRp7fg as they are not in the training set.\n",
      "Skipping refinement for ezFwl_4j7dzitHnFUmkPHg as they are not in the training set.\n",
      "Skipping refinement for fBkMdxKevYG1ALZbpjyu6g as they are not in the training set.\n",
      "Skipping refinement for fNfW1b4VMX2bCzbNMsoY6A as they are not in the training set.\n",
      "Skipping refinement for fSfegMflm7DHSXUHBOmz5A as they are not in the training set.\n",
      "Skipping refinement for fhISINkVlcwaTgzrM75H1w as they are not in the training set.\n",
      "Skipping refinement for g1LkJdSXJJba3X-NK6Dm4A as they are not in the training set.\n",
      "Skipping refinement for gGgFKHjBup0wPNmxJH6R2g as they are not in the training set.\n",
      "Skipping refinement for gMfiGKtpVIi99vDlqVgT-g as they are not in the training set.\n",
      "Skipping refinement for gZN4aYXU2zQVAkS81Eeg4Q as they are not in the training set.\n",
      "Skipping refinement for gekp2Z5I4vPGf2NeFnZX2A as they are not in the training set.\n",
      "Skipping refinement for gnByrfCawD-btFTmSlHQwg as they are not in the training set.\n",
      "Skipping refinement for h5e3Lt-rkc4l0rL_J31XHQ as they are not in the training set.\n",
      "Skipping refinement for h6CBkFkvVe1AXtwzz3QKIg as they are not in the training set.\n",
      "Skipping refinement for h6SY_r3T3iEUEeweWxaYBA as they are not in the training set.\n",
      "Skipping refinement for h7DbYWJyknQfLX9cbwmxJQ as they are not in the training set.\n",
      "Skipping refinement for hGeXKYWkqJgFC1u_Sh6kCg as they are not in the training set.\n",
      "Skipping refinement for hRzfvXQgMUV2IXn3hdUJoA as they are not in the training set.\n",
      "Skipping refinement for hSbCSR7F8HAwfoZcm0jpww as they are not in the training set.\n",
      "Skipping refinement for hk6B1ZdzI62jMGo6e2IuuA as they are not in the training set.\n",
      "Skipping refinement for hktq74cO458J8FcVZhkSqQ as they are not in the training set.\n",
      "Skipping refinement for hnPTPyh6wNfZe1GxOm21Gg as they are not in the training set.\n",
      "Skipping refinement for htKk-h8d3NaZVAQLLJidKw as they are not in the training set.\n",
      "Skipping refinement for i1Oxr_XAscirXOB5F4Pacg as they are not in the training set.\n",
      "Skipping refinement for iB1klKQYi9MBdzp2DbOoJA as they are not in the training set.\n",
      "Skipping refinement for iJpgy47yC8tpwy7_6G3tpA as they are not in the training set.\n",
      "Skipping refinement for iM0ZCnX-2jLX98hyCFQqPA as they are not in the training set.\n",
      "Skipping refinement for iP3bGNFBJjPGLxB8xG-RdA as they are not in the training set.\n",
      "Skipping refinement for iYk1CiIWlekL4mRIoMA0RA as they are not in the training set.\n",
      "Skipping refinement for iZyd2oof4Do6j19jnlKhuw as they are not in the training set.\n",
      "Skipping refinement for itZpXOXJoLLEGJxL_17aig as they are not in the training set.\n",
      "Skipping refinement for ivp_DM9umXXYhJ41hw0VKw as they are not in the training set.\n",
      "Skipping refinement for jFckZv_dAGGvN2Uo_pfLJg as they are not in the training set.\n",
      "Skipping refinement for jLBBvQ6Yoi_ZtAMZxYiQdA as they are not in the training set.\n",
      "Skipping refinement for jNGgI4YQLx9dSpJ3zzwPQg as they are not in the training set.\n",
      "Skipping refinement for jkybaeTS5cCGSDeJafxWzQ as they are not in the training set.\n",
      "Skipping refinement for k3Xxkt-rplj8-kDpwQ33MQ as they are not in the training set.\n",
      "Skipping refinement for k8y8sNAXTl6XMzg5Qr61rg as they are not in the training set.\n",
      "Skipping refinement for kZFPKu-N0vTcy0Oc6rmG7Q as they are not in the training set.\n",
      "Skipping refinement for kynpayfZunwfm8bMR8fRZw as they are not in the training set.\n",
      "Skipping refinement for l0e_QnB17xqzv5BAFOqiGQ as they are not in the training set.\n",
      "Skipping refinement for lPlxvl8DB1ETPRXG7T3sIg as they are not in the training set.\n",
      "Skipping refinement for lSKKtZBmD0GIYbirCMUKtw as they are not in the training set.\n",
      "Skipping refinement for l_Nq9zZ-Ks0ncDgSQ4tH8Q as they are not in the training set.\n",
      "Skipping refinement for liZD2cypFsWBYzokc8vG9w as they are not in the training set.\n",
      "Skipping refinement for lq5wYP--UYHHEMLSo5NM9A as they are not in the training set.\n",
      "Skipping refinement for m0UxagN9K80ZvXqp3GBa9A as they are not in the training set.\n",
      "Skipping refinement for m2kW7KeMuZCwtTEH--Od9g as they are not in the training set.\n",
      "Skipping refinement for mFBnOCP92E6A738_FqYQwg as they are not in the training set.\n",
      "Skipping refinement for mF_3TEQpor7IkT4656M6Ng as they are not in the training set.\n",
      "Skipping refinement for mcJ-A0OGmiWZGBbdWPMfrw as they are not in the training set.\n",
      "Skipping refinement for mo5cD3-ehbhWGz7ExhpaNA as they are not in the training set.\n",
      "Skipping refinement for mpI4O2jkt7XN8byfpE1omg as they are not in the training set.\n",
      "Skipping refinement for n1OrzdUywZXcvrRa7bfQag as they are not in the training set.\n",
      "Skipping refinement for nCxP-GJ7WYg1GUPlwf3PxQ as they are not in the training set.\n",
      "Skipping refinement for nMr9HrFjvMl5TvEC70nf_g as they are not in the training set.\n",
      "Skipping refinement for nNrdU3_w8KPFasq0RCPlvw as they are not in the training set.\n",
      "Skipping refinement for nYXjXuFswfovytDQCcxN4A as they are not in the training set.\n",
      "Skipping refinement for ncWuUSITGNp8Q2-WHjtCLw as they are not in the training set.\n",
      "Skipping refinement for njUgJcn8ZN0BdZdNEswRwA as they are not in the training set.\n",
      "Skipping refinement for nyJ5KVXhcpX6RDAHT36Cwg as they are not in the training set.\n",
      "Skipping refinement for o0qek79Gl5j2hM8WJM5OQg as they are not in the training set.\n",
      "Skipping refinement for oEq6jLdSR7Yeno1xFdlkcQ as they are not in the training set.\n",
      "Skipping refinement for oGhFfL-ag6SYT2v1L-SakQ as they are not in the training set.\n",
      "Skipping refinement for oQ7S1eaUyFCYEHhWdGa92A as they are not in the training set.\n",
      "Skipping refinement for oZ_pZtv8ARgBlfOWdEWlbA as they are not in the training set.\n",
      "Skipping refinement for oczRG9N-sLcCEIIcsGylqA as they are not in the training set.\n",
      "Skipping refinement for ohEm5Cu_2GrK1yKh1cFD3A as they are not in the training set.\n",
      "Skipping refinement for oltnRw2YjyVXmfxc5ilWMA as they are not in the training set.\n",
      "Skipping refinement for op2c4RQMptf9pi5KoNltWw as they are not in the training set.\n",
      "Skipping refinement for pBPc0ingZ7ryynu5Y7ON-w as they are not in the training set.\n",
      "Skipping refinement for pNF7ShX5v_BqbRNe0mFCUw as they are not in the training set.\n",
      "Skipping refinement for pmXiaK7RM_veqJ8OcGhRJw as they are not in the training set.\n",
      "Skipping refinement for pv-q73AyQSG_nLXSziZxwA as they are not in the training set.\n",
      "Skipping refinement for q-qZF6EmDg8_zuVXAyVcJg as they are not in the training set.\n",
      "Skipping refinement for q2Xcpp_5oAASImqVomwiPg as they are not in the training set.\n",
      "Skipping refinement for qPMcmtVX-VcMcvrIRK8-fw as they are not in the training set.\n",
      "Skipping refinement for qR5ENbNeUgspQydJZH_8ww as they are not in the training set.\n",
      "Skipping refinement for qV-1hF9oBm1ViSTnm45pFA as they are not in the training set.\n",
      "Skipping refinement for qVx3I3sqkjk7-YsOQHeNiA as they are not in the training set.\n",
      "Skipping refinement for qgCanTXLz_myQ8VTMLApGg as they are not in the training set.\n",
      "Skipping refinement for qifgGoyS3Fnt7KogFa064Q as they are not in the training set.\n",
      "Skipping refinement for qlD7FffWlQmBO597CeLHtQ as they are not in the training set.\n",
      "Skipping refinement for r1WefBOIn1q72UEzUk5Ryg as they are not in the training set.\n",
      "Skipping refinement for rJCbNpqG7ew3JoF5raqYog as they are not in the training set.\n",
      "Skipping refinement for rYnVJvMUhCtEK-JNTG7mfA as they are not in the training set.\n",
      "Skipping refinement for raBtex93rDi2LXMgXeHa9Q as they are not in the training set.\n",
      "Skipping refinement for rcxQVKzWrSuIp3ygUENB1w as they are not in the training set.\n",
      "Skipping refinement for rwzqcq7p4xx7bCX3qGHT8A as they are not in the training set.\n",
      "Skipping refinement for rxNku8x9n5F68t0RF266yw as they are not in the training set.\n",
      "Skipping refinement for sCOjoM_buXFVrnAFLVYktA as they are not in the training set.\n",
      "Skipping refinement for sTtptFLJ7KdyE_sTi5PhQQ as they are not in the training set.\n",
      "Skipping refinement for scMLUgV1SzTe9TjBBuf3tg as they are not in the training set.\n",
      "Skipping refinement for siikoDWFY6YLctJE1EJ7Ow as they are not in the training set.\n",
      "Skipping refinement for t8Wd3y8VeEzww0nuJ0hhSw as they are not in the training set.\n",
      "Skipping refinement for t9OAdJcdL3suwm0jsAzQEw as they are not in the training set.\n",
      "Skipping refinement for tLc0wyimcpZbfhc-ozdUDw as they are not in the training set.\n",
      "Skipping refinement for tfup6bDT1GhRDqDqrruXiA as they are not in the training set.\n",
      "Skipping refinement for u-Oa9itidP-Nu2g5N_Mr5Q as they are not in the training set.\n",
      "Skipping refinement for uncpOYLlTS_g6GRgE6Rnuw as they are not in the training set.\n",
      "Skipping refinement for v8svU5JLgN-Ega0nRbRS1A as they are not in the training set.\n",
      "Skipping refinement for vXQCqyBzjCF3192JOF6oaw as they are not in the training set.\n",
      "Skipping refinement for vf5heRma4NAPhhcKrnc7Cg as they are not in the training set.\n",
      "Skipping refinement for vrPBIr1rQxQk3rarehBEig as they are not in the training set.\n",
      "Skipping refinement for w0f3twDIXtkYuKUzfy_s5A as they are not in the training set.\n",
      "Skipping refinement for w4PqT-9vVSVpamDvTHnsDw as they are not in the training set.\n",
      "Skipping refinement for wEldYiMRs04ykBXBmDXdBw as they are not in the training set.\n",
      "Skipping refinement for wLr4wi0YKU52U3mN3Ri79w as they are not in the training set.\n",
      "Skipping refinement for wb1rCDV-4lGX1ro7eC1pXQ as they are not in the training set.\n",
      "Skipping refinement for wmOV_IMAulmzk6qkUnmOhg as they are not in the training set.\n",
      "Skipping refinement for xK_6Hl0uOO3KUM6P9GQ8Kg as they are not in the training set.\n",
      "Skipping refinement for x_Frpa8QWc25FDJ5BJM8tg as they are not in the training set.\n",
      "Skipping refinement for xgcbjcJvhvSf6zgw9DuUkw as they are not in the training set.\n",
      "Skipping refinement for xqmyAQXVBUQ4JvdG2T8XLA as they are not in the training set.\n",
      "Skipping refinement for xrpwUo4mJkjq6RFwhZhCoA as they are not in the training set.\n",
      "Skipping refinement for xxblDBC1-arq8m2snrkiFQ as they are not in the training set.\n",
      "Skipping refinement for y5JJVlWfEZoDhafzomVgSA as they are not in the training set.\n",
      "Skipping refinement for y94FS7iXvZp9QISUc0F__Q as they are not in the training set.\n",
      "Skipping refinement for y9bMBKNvoHV2xo1UTACJRw as they are not in the training set.\n",
      "Skipping refinement for yA9lR4NXtsfTqpBS9TnzUA as they are not in the training set.\n",
      "Skipping refinement for yFtXeU4z-bdV3czVt4pD8A as they are not in the training set.\n",
      "Skipping refinement for yT_QCcnq-QGipWWuzIpvtw as they are not in the training set.\n",
      "Skipping refinement for yWP994Krdcga9ZB5Kh1krQ as they are not in the training set.\n",
      "Skipping refinement for yYSc16a8eY-Rt-j7sUbGYw as they are not in the training set.\n",
      "Skipping refinement for ybrLqhrVUr5nwN0HNv2sYw as they are not in the training set.\n",
      "Skipping refinement for yeUN85YNAtJ3ORnkkGFALw as they are not in the training set.\n",
      "Skipping refinement for ynOKdr3bfj9MvTwhfis2rw as they are not in the training set.\n",
      "Skipping refinement for ytTr7hwn6fUpydCc4WZC3A as they are not in the training set.\n",
      "Skipping refinement for z3n4lF8oWYahp-3Hx56d7A as they are not in the training set.\n",
      "Skipping refinement for zQT7oLYJcT94GSHcEC_ELQ as they are not in the training set.\n",
      "Skipping refinement for zmMUoDcYC3DmbUfia64pHQ as they are not in the training set.\n",
      "Skipping refinement for zs_leBrW0h5HueGaogo_Ig as they are not in the training set.\n",
      "Skipping refinement for zwCkk1Pd88uZpvIQRRFo-A as they are not in the training set.\n",
      "Skipping refinement for zzBhA0M7NNBMYoWj48h53A as they are not in the training set.\n"
     ]
    }
   ],
   "source": [
    "def refine_with_item_based(top_n_recommendations, item_based_algo, trainset):\n",
    "    final_recommendations = defaultdict(list)\n",
    "    for uid, items in top_n_recommendations.items():\n",
    "        # Check if the user is in the training set\n",
    "        try:\n",
    "            inner_uid = trainset.to_inner_uid(uid)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping refinement for {uid} as they are not in the training set.\")\n",
    "            continue\n",
    "\n",
    "        item_based_predictions = []\n",
    "        for iid, _ in items:\n",
    "            try:\n",
    "                inner_iid = trainset.to_inner_iid(iid)\n",
    "                # Only predict if both user and item are in the training set\n",
    "                est = item_based_algo.predict(uid, iid, verbose=False).est\n",
    "                item_based_predictions.append((iid, est))\n",
    "            except ValueError:\n",
    "                # Item not in training set, skip it\n",
    "                continue\n",
    "\n",
    "        item_based_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_recommendations[uid] = item_based_predictions[:5]\n",
    "\n",
    "    return final_recommendations\n",
    "\n",
    "\n",
    "final_recommendations = refine_with_item_based(top_n_user_based, item_based_algo, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a97343d5-0cbd-46fd-b421-e3e9f05662a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5 (with ties): 0.6576055523423945\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_at_k_with_ties(final_recommendations, testset, k=5):\n",
    "    # Step 1: Prepare the data structures\n",
    "    test_ratings_by_user = defaultdict(list)\n",
    "    for uid, iid, rating in testset:\n",
    "        test_ratings_by_user[uid].append((iid, rating))\n",
    "\n",
    "    top_5_items_with_ties_by_user = defaultdict(set)\n",
    "    for uid, items_ratings in test_ratings_by_user.items():\n",
    "        sorted_items = sorted(items_ratings, key=lambda x: x[1], reverse=True)\n",
    "        top_ratings = sorted_items[:k] if len(sorted_items) > k else sorted_items\n",
    "        min_top_rating = min(top_ratings, key=lambda x: x[1])[1]\n",
    "        top_5_items_with_ties_by_user[uid] = {item for item, rating in sorted_items if rating >= min_top_rating}\n",
    "\n",
    "    # Step 2: Calculate precision\n",
    "    hits = 0\n",
    "    total_predictions = 0\n",
    "    for uid, recommendations in final_recommendations.items():\n",
    "        recommended_item_ids = {iid for iid, _ in recommendations[:5]}\n",
    "        actual_top_items = top_5_items_with_ties_by_user.get(uid, set())\n",
    "        \n",
    "        hits += len(recommended_item_ids.intersection(actual_top_items))\n",
    "        total_predictions += len(recommended_item_ids)\n",
    "\n",
    "    precision_at_k_with_ties = hits / total_predictions if total_predictions > 0 else 0\n",
    "    return precision_at_k_with_ties\n",
    "\n",
    "# Usage example\n",
    "precision = calculate_precision_at_k_with_ties(final_recommendations, testset, k=5)\n",
    "print(f\"Precision@5 (with ties): {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb1d0b-3306-4d97-9de5-6abd93548e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
